{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ptt_get.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/williamshiaogit/DL-ML-AI-learning/blob/master/ptt_get.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhO8W14mmJmi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "\n",
        "\n",
        "def get_date_str(bias=0):\n",
        "    \"\"\"\n",
        "    get the date as a string only shows month/day\n",
        "    [bias] * as days want to add on today(could be minus)\n",
        "           * defaut is zero stands for today\n",
        "    \"\"\"\n",
        "    today = datetime.datetime.today()  # 獲得今天的日期\n",
        "    date = (today + datetime.timedelta(days=bias)).strftime(\"%m/%d\")  # 格式化日期\n",
        "    return ' ' + date[1:] if date[0] == '0' else date     # 把0換成空白\n",
        "\n",
        "# print(get_date_str())       # today\n",
        "# print(get_date_str(-3))     # 3 days ago\n",
        "# print(get_date_str(-7))     # 7 days ago\n",
        "# print(get_date_str(100))    # day after 100 days\n",
        "\n",
        "\n",
        "def Is_within_Target_Date_2020(this_date, target_date):\n",
        "    # replace ' ' by '0' for the first letter\n",
        "    this_date = '0' + this_date[1:] if this_date[0] == ' ' else this_date\n",
        "    target_date = '0' + target_date[1:] if target_date[0] == ' ' else target_date\n",
        "\n",
        "    this = datetime.date(2020, int(this_date[0:2]), int(this_date[3:]))\n",
        "    target = datetime.date(2020, int(target_date[0:2]), int(target_date[3:]))\n",
        "\n",
        "    # print for debugging\n",
        "    # print(\"{} >= {} : {}\" .format(this.strftime(\"%m/%d\"), target.strftime(\"%m/%d\"), this >= target))\n",
        "\n",
        "    return this >= target\n",
        "\n",
        "# Is_within_Target_Date_2020('03/12', '03/11')\n",
        "# Is_within_Target_Date_2020('03/11', '03/11')\n",
        "# Is_within_Target_Date_2020('03/10', '03/11')\n",
        "# Is_within_Target_Date_2020(get_date_str(), get_date_str(-7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glVagObNmLZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "#from day_computing import get_date_str, Is_within_Target_Date_2020\n",
        "\n",
        "\n",
        "def ptt_listpage_crawler(url, day_bias=0):\n",
        "    \"\"\"\n",
        "    crawl the ptt list page\n",
        "\n",
        "    para::[url]\n",
        "        - type: str\n",
        "        - starting page URL\n",
        "    para::[day_bias]\n",
        "        - type: (minus) int\n",
        "        - number of days before today\n",
        "        - example: zero stands for today; -1 stands for yesterday \n",
        "    return::\n",
        "        - [str] url of previous page; return None if achieved target date\n",
        "        - [list] posts collection with target date\n",
        "    \"\"\"\n",
        "\n",
        "    jar = requests.cookies.RequestsCookieJar()\n",
        "    # 可把不同網頁的 cookie 設定進一個jar\n",
        "    jar.set(\"over18\", \"1\", domain=\"www.ptt.cc\")\n",
        "    # 將cookies加入request\n",
        "    response = requests.get(url, cookies=jar).text\n",
        "    # response為html格式，交由bs4解析\n",
        "    html = bs(response)\n",
        "\n",
        "    # 找到導航列\n",
        "    navi_bar = html.find(\"div\", class_=\"btn-group btn-group-paging\")\n",
        "    navi_bottons = navi_bar.find_all(\"a\", class_=\"btn wide\")\n",
        "    # 從導航列取得上一頁的URL(next_url)\n",
        "    for n in navi_bottons:\n",
        "        if '上頁' in n.text:\n",
        "            next_url = \"https://www.ptt.cc\" + n[\"href\"]\n",
        "\n",
        "\n",
        "    post_block = html.find(\"div\", class_=\"r-list-container action-bar-margin bbs-screen\")\n",
        "    posts = post_block.find_all(\"div\", class_=\"r-ent\")\n",
        "    post_list = []\n",
        "    for post in posts:\n",
        "        p = {}\n",
        "        # Title - 文章標題\n",
        "        try:\n",
        "            p['title'] = post.find(\"div\", class_=\"title\").find(\"a\").text\n",
        "        except AttributeError:      # 如果文章已被刪除則略過\n",
        "            p['title'] = post.find(\"div\", class_=\"title\").text\n",
        "            if '刪除' in p['title']:\n",
        "                continue\n",
        "        if '公告' in p['title']:    # 如果是公告文則略過\n",
        "            continue\n",
        "        # URL - 文章連結\n",
        "        try:\n",
        "            url_path = post.find(\"div\", class_=\"title\").find(\"a\")[\"href\"]\n",
        "            p['url'] = \"https://www.ptt.cc\" + url_path\n",
        "            p['slug'] = '.'.join(url_path.split('/')[3].split('.')[:4])\n",
        "            \n",
        "        except:\n",
        "            continue\n",
        "        # DATE - 日期\n",
        "        p['post_time'] = post.find(\"div\", class_=\"date\").text\n",
        "        if not Is_within_Target_Date_2020(p['post_time'], get_date_str(day_bias)):\n",
        "            next_url = None \n",
        "            continue  # 不在目標日期範圍內，略過此筆\n",
        "        # PUSH - 推文數\n",
        "        try:\n",
        "            p['push'] = post.find(\"div\", class_=\"nrec\").find(\"span\", class_=\"hl\").text\n",
        "        except AttributeError:      # 處理沒有人推文\n",
        "            p['push'] = 0\n",
        "        post_list.append(p)\n",
        "\n",
        "    return next_url, post_list\n",
        "\n",
        "# # Print for debugging\n",
        "# for p in post_list:\n",
        "#     for key, value in p.items():\n",
        "#         print(\"{}: {}\" .format(key, value))\n",
        "#     print(\"------------------------------------------------\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cbYa7WNmLhM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from urllib.request import urlretrieve\n",
        "from zipfile import ZipFile\n",
        "from os import remove\n",
        "import ssl\n",
        "import warnings\n",
        "\n",
        "\n",
        "# Issue handling\n",
        "ssl._create_default_https_context = ssl._create_unverified_context  # SSL\n",
        "warnings.filterwarnings('ignore')  # 移除bs4的warning\n",
        "\n",
        "\n",
        "def post_crawler(post_url, zipfile_name):\n",
        "    jar = requests.cookies.RequestsCookieJar()\n",
        "    # 可把不同網頁的 cookie 設定進一個jar\n",
        "    jar.set(\"over18\", \"1\", domain=\"www.ptt.cc\")\n",
        "    # 將cookies加入request\n",
        "    response = requests.get(post_url, cookies=jar).text\n",
        "\n",
        "    # response為html格式，交由bs4解析\n",
        "    html = bs(response)\n",
        "\n",
        "    main_content = html.find(\"div\", class_=\"bbs-screen bbs-content\")\n",
        "\n",
        "    metas = main_content.find_all(\"span\", class_=\"article-meta-tag\")\n",
        "    m_values = main_content.find_all(\"span\", class_=\"article-meta-value\")\n",
        "\n",
        "    # Remove(extract) 作者 標題 時間 ------------------------\n",
        "    meta = main_content.find_all(\"div\", class_=\"article-metaline\")\n",
        "    for m in meta:\n",
        "        m.extract()\n",
        "    # Remove(extract) 看板名稱 ------------------------------\n",
        "    right_meta = main_content.find_all(\"div\", class_=\"article-metaline-right\")\n",
        "    for single_meta in right_meta:\n",
        "        single_meta.extract()\n",
        "\n",
        "    # Remove(extract) 推文前   ------------------------------\n",
        "    datas = main_content.find_all(\"span\", class_=\"f2\")\n",
        "    for data in datas:\n",
        "        data.extract()\n",
        "\n",
        "    # Remove(extract) 推文   --------------------------------\n",
        "    pushes = main_content.find_all(\"div\", class_=\"push\")\n",
        "    comments = []\n",
        "    for single_push in pushes:\n",
        "        push_tag = single_push.find(\"span\", class_=\"push-tag\").text.strip()\n",
        "        push_userid = single_push.find(\"span\", class_=\"push-userid\").text.strip()\n",
        "        push_content = single_push.find(\"span\", class_=\"push-content\").text.split(\":\")[1].strip()\n",
        "        ipdatetime = single_push.find(\"span\", class_=\"push-ipdatetime\").text\n",
        "        push_ip = ipdatetime.strip().split(\" \")[0]\n",
        "        push_time = \" \".join(ipdatetime.strip().split(\" \")[1:])\n",
        "        comment = { \"status\": push_tag, \"comment_id\": push_userid, \"content\": push_content,\"ip\": push_ip,\"comment_time\": push_time }\n",
        "        comments.append(comment)\n",
        "\n",
        "    # Remove(extract) imgur圖片 ----------------------------\n",
        "    # 1. 第一個部分，連結\n",
        "    img_l = []\n",
        "    photo_hrefs = main_content.find_all(\"a\")\n",
        "    for pic in photo_hrefs:\n",
        "        if 'imgur' in pic[\"href\"] and 'https' in pic[\"href\"]:\n",
        "            img_l.append(pic[\"href\"])\n",
        "            pic.extract()\n",
        "    # # 2. 第二個部分，圖片顯示(richcontent)\n",
        "    # richcontents = main_content.find_all(\"div\", class_=\"richcontent\")\n",
        "    # for rich in richcontents:\n",
        "    #   rich.extract()\n",
        "\n",
        "    reply_msg = \"\"  # 開始搜集回應使用者的文字資訊\n",
        "\n",
        "    # for (m, v) in zip(metas, m_values):\n",
        "    #     print(m.text, ':', v.text)\n",
        "    # for (m, v) in zip(metas, m_values):\n",
        "    #     if m.text in ['標題', '時間']:\n",
        "    #         reply_msg += m.text + ': ' + v.text + '\\n'\n",
        "    #     print(m.text, ':', v.text)\n",
        "    # print(m_values[0].text)\n",
        "    try:\n",
        "        author = m_values[0].text        \n",
        "    except:\n",
        "        author = ''\n",
        "    \n",
        "    try:\n",
        "        post_time = m_values[3].text\n",
        "    except:\n",
        "        post_time = ''\n",
        "\n",
        "    # print(\"分數 :\", score)\n",
        "    # reply_msg += \"分數: \" + str(score) + '\\n'\n",
        "    # print(\"內文 :\")\n",
        "    # reply_msg += \"內文: \"\n",
        "\n",
        "    # print(main_content.text)\n",
        "    content = main_content.text\n",
        "    # content_split = content.split('--')\n",
        "    origin_content = content.split('--')[0]\n",
        "    ori_l = origin_content.split(\"\\n\")\n",
        "    ori_linted = []\n",
        "    for o in ori_l:\n",
        "        if o != \"\":\n",
        "            ori_linted.append(o)\n",
        "    # for i in ori_linted:\n",
        "    #     reply_msg += (i + '\\n')\n",
        "    #     print(i)\n",
        "\n",
        "    # print(\"圖片連結 :\")\n",
        "    img_name_list = []\n",
        "    img_url_list = []\n",
        "    for img in img_l:\n",
        "        # 連結不含副檔名則加上.jpg\n",
        "        if img[-4:] != \".gif\" and img[-3:] != \"jpg\":\n",
        "            img += \".jpg\"\n",
        "        # 下載圖片\n",
        "        f = img.split('/')[-1]\n",
        "        # urlretrieve(img, f)\n",
        "        # print(img)\n",
        "        img_name_list.append(f)\n",
        "        img_url_list.append(img)\n",
        "\n",
        "    return post_time, author, comments, img_name_list, img_url_list  # (回應使用者的文字資訊, 回傳圖片檔名list)\n",
        "\n",
        "\n",
        "def img_dl_zip(img_url_list, zipfile_name):\n",
        "    f_name = zipfile_name + '.zip'\n",
        "    with ZipFile(f_name, 'w') as myzip:\n",
        "        for img in img_url_list:\n",
        "            # 連結不含副檔名則加上.jpg\n",
        "            if img[-4:] != \".png\" and img[-4:] != \".gif\" and img[-3:] != \"jpg\":\n",
        "                img += \".jpg\"\n",
        "\n",
        "            # 下載圖片、加入zip、刪除照片\n",
        "            f = img.split('/')[-1]\n",
        "            urlretrieve(img, f)\n",
        "            print(\"downloaded: \", img)\n",
        "\n",
        "            filename = img.split('/')[-1]\n",
        "            myzip.write(filename)\n",
        "            print(\"added to zip: \", filename)\n",
        "\n",
        "            remove(filename)\n",
        "            print(\"removed: \", filename)\n",
        "    return f_name\n",
        "\n",
        "\n",
        "def rm_img(img_name_list, num):\n",
        "    '''\n",
        "    param img_name_list: image file names\n",
        "    param num: number of img to remove this time\n",
        "                1, 2, 3, ...\n",
        "                0 for remove all\n",
        "    '''\n",
        "    if num == 0:\n",
        "        for img in img_name_list:\n",
        "            print(img)\n",
        "            remove(img)\n",
        "    else:\n",
        "        for _ in range(num):\n",
        "            f = img_name_list.pop(0)\n",
        "            remove(f)\n",
        "            print(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEpBGoiXmLoZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from time import sleep, time\n",
        "#from listpage_crawler import ptt_listpage_crawler\n",
        "#from postpage_crawler import post_crawler\n",
        "import json\n",
        "\n",
        "# 取得指定天數的 post list 資料，存成陣列\n",
        "def get_post_list(day_bias = 7):\n",
        "    # Input day_bias: 要查詢的天數(正整數)\n",
        "    # return msg_r : 可傳給使用者看的查詢結果(string)\n",
        "    # return df_r : 查詢結果組成的sorted DataFrame\n",
        "    # PTT表特版首頁\n",
        "    url = \"https://www.ptt.cc/bbs/Beauty/index.html\"\n",
        "\n",
        "    day_bias = -day_bias + 1\n",
        "\n",
        "    start_time = time()\n",
        "\n",
        "    post_list = []\n",
        "    delay_sec = 0.1\n",
        "    count = 0\n",
        "    if day_bias > 0:\n",
        "        print(\"Check day_bias!!!\")\n",
        "    # while url != None:\n",
        "    while url is not None:\n",
        "        url, next_post_list = ptt_listpage_crawler(url, day_bias)\n",
        "        print(url,'\\n')\n",
        "        post_list = post_list + next_post_list[::-1]\n",
        "        count += 1\n",
        "        sleep(delay_sec)\n",
        "\n",
        "    end_time = time()\n",
        "    duration = end_time - start_time\n",
        "    print(\"花費時間: \",duration)\n",
        "    return post_list\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kyzlSa_wntC",
        "colab_type": "text"
      },
      "source": [
        "** 這裏可以設定爬的天數 N **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRPSOdkDoGMf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "02594ac5-17f9-4b14-a1be-653ad599b10a"
      },
      "source": [
        "n=1 #調整爬的天數\n",
        "\n",
        "post_list = get_post_list(n)\n",
        "\n",
        "print(\"共爬: \", n ,\" 天\")\n",
        "\n",
        "count=0\n",
        "for index, post in enumerate(post_list):\n",
        "    post_time, author, comments, img_name_list, img_url_list = post_crawler(post['url'], index)\n",
        "    post_list[index]['post_time'] = post_time\n",
        "    post_list[index]['imgs'] = img_url_list\n",
        "    post_list[index]['author'] = author\n",
        "    post_list[index]['comments'] = comments\n",
        "    count+=1\n",
        "    \n",
        "    # 顯示每一頁的資料\n",
        "    #print(post_list[index]['url'])\n",
        "    #print(post_list[index],'\\n')\n",
        "\n",
        "print(\"\\n==============================\\n\")\n",
        "print(\"\\n爬取po文資料 共\",count, \"筆\\n\")\n",
        "\n",
        "# 直接暫存json 讀取\n",
        "with open('data.json', 'w') as file:\n",
        "    json.dump(post_list, file, ensure_ascii=False)\n",
        "\n",
        "import json\n",
        "with open('data.json') as json_file:\n",
        "    jj = json.load(json_file)\n",
        "\n",
        "count1=0\n",
        "for p in jj:\n",
        "    for img in p['imgs']:\n",
        "        if img.split(\".\")[-1] == \"jpg\":\n",
        "             try:\n",
        "                count1 +=1\n",
        "             except:\n",
        "                pass\n",
        "        else:\n",
        "            continue\n",
        "print(\"預計儲存所有圖片共：\",count1,\"張\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://www.ptt.cc/bbs/Beauty/index3394.html \n",
            "\n",
            "None \n",
            "\n",
            "花費時間:  0.8218824863433838\n",
            "共爬:  1  天\n",
            "\n",
            "==============================\n",
            "\n",
            "\n",
            "爬取po文資料 共 12 筆\n",
            "\n",
            "預計儲存所有圖片共： 111 張\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "579_3SBImnKU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}